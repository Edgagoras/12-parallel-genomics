{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Python -- multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "import ipyparallel as ipp\n",
    "import requests\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel processing involves farming jobs out to different processors on your machine. All modern laptops have multiple processors that can be used to run tasks in parallel. However, writing parallel code can be tricky, and it often requires some understanding of how parallel processing works in order to write code to use it efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor IDs\n",
    "The `pid` of a process is the processor ID. This is simply a number that is assigned to a process when it is started. You can think of a process as a reservation on one of your computer's CPUs. It is being reserved for a set task. When we run parallel code with 4 processes the work should be farmed out to four different `pids` that will run on different CPUs. Below is a very simple function that we will use just to check that our code is working the way that we expect. All this function does is tell `sleep` for 1 second, and then return the value of its `pid`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def who(i):\n",
    "    time.sleep(i)\n",
    "    return os.getpid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (1) Run the who function on a single processor\n",
    "As expected, this takes about 1 second to complete, and return a number corresponding to the process it was run on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7209"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run single/normal function\n",
    "who(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (2) Run the who function on multiple processors\n",
    "Here we will submit 10 jobs, one for each item in `queries`, and run the `who()` function on each. We would expect that on a single processor this should take at least 10 seconds to run if we use a 1 second sleep time, but because we're running it on 4 processors in parallel it should take less time. As you can see in the output, each of the four available processors runs a job (i.e., the same `pid` is returned several times). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7223, 7224, 7225, 7226, 7226, 7223, 7224, 7225, 7226, 7223]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    \n",
    "    # submit queries to threads\n",
    "    jobs = [pool.submit(who, 1) for q in queries]\n",
    "    \n",
    "    # collect results\n",
    "    results = [i.result() for i in jobs]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProcessPool and Multiprocessing are very similar. \n",
    "These are two different syntaxes for doing multiprocessing. The former is new to Python3, the latter works in Py2 or Py3 and is more commonly used currently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.05 s ± 10.5 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 5\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    \n",
    "    # submit queries to threads\n",
    "    jobs = [pool.submit(who, 1) for q in queries]\n",
    "    \n",
    "    # collect results\n",
    "    results = [i.result() for i in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.04 s ± 3.2 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 5\n",
    "with mp.Pool(processes=4) as pool:\n",
    "    \n",
    "    # submit queries to threads (with args as tuples)\n",
    "    jobs = [pool.apply_async(who, [1]) for q in queries]\n",
    "    \n",
    "    # collect results\n",
    "    results = [i.get() for i in jobs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Pool` object\n",
    "In the example above we use a `with` context manager to create a `Pool` object. The object is what interfaces with your CPUs (processes) to send inputs and receive outputs of function calls. There are several ways to send and receive jobs, but the most simple and understandable is the method called `apply_async` for `multiprocessing`, or `submit` for ProcessPoolExecutor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7871, 7872, 7873, 7874]\n"
     ]
    }
   ],
   "source": [
    "# apply_async returns the result of a function call when requested, and \n",
    "# does not block you from continuing to execute code while it is running.\n",
    "# This is true parallel processing.\n",
    "with mp.Pool(processes=4) as pool:\n",
    "    a = pool.apply_async(who, [1])\n",
    "    b = pool.apply_async(who, [1])\n",
    "    c = pool.apply_async(who, [1])\n",
    "    d = pool.apply_async(who, [1])\n",
    "    print([i.get() for i in (a, b, c, d)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `AsyncResult` object\n",
    "The object that is return by an `apply_async()` function call is called an `AsyncResult`. This object is used to query the job that we sent to a different processor to ask whether it is finished yet, and to collect the results when it is done. The four main functions to call from \"async\" objects are `wait`, `get`, `ready` and `successful`. The first function, `wait`, tells the object to `block` until the job is finished, meaning you will no longer be able to run code while it is running. The `get` call is used to ask for a result. It will also `block` if you ask for a result and the job is not finished yet. The `ready` statement is useful because you can ask whether a job is ready yet and it does not `block` when you ask. And finally, the `successful` function will return whether the job completed or raised an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of the `get()` and `wait()` functions.\n",
    "The `wait()` function is called inside of the `get()` function, `wait` is only used if you want to block anything else from happening, such as other jobs from being submitted, until the `get` function can be called, meaning that results can be returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job was submitted\n",
      "waiting for result...\n",
      "executing print statements in this notebook while waiting...\n",
      "job finished\n"
     ]
    }
   ],
   "source": [
    "# start a pool\n",
    "with mp.Pool(processes=4) as pool:\n",
    "\n",
    "    # this does NOT BLOCK and so the code below is executed while \n",
    "    # the async job is running on a different processor\n",
    "    asyncr = pool.apply_async(time.sleep, [2])\n",
    "    print('job was submitted')\n",
    "    print('waiting for result...')\n",
    "    print('executing print statements in this notebook while waiting...')\n",
    "\n",
    "    # collect result\n",
    "    asyncr.get()\n",
    "    print('job finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job was submitted\n",
      "job finished; this couldn't be printed until it was finished\n"
     ]
    }
   ],
   "source": [
    "# start a pool\n",
    "with mp.Pool(processes=4) as pool:\n",
    "\n",
    "    # this DOES BLOCK and so the code below is NOT executed \n",
    "    # while the async job is running on a different processor,\n",
    "    # but instead only executes after the job is finished.\n",
    "    asyncr = pool.apply_async(time.sleep, [2])\n",
    "    print('job was submitted')\n",
    "\n",
    "    asyncr.wait()\n",
    "\n",
    "    # collect result\n",
    "    asyncr.get()\n",
    "    print(\"job finished; this couldn't be printed until it was finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of the `ready()` and `successful()` functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<multiprocessing.pool.ApplyResult object at 0x7f7a58dcedd8> had an error\n",
      "<multiprocessing.pool.ApplyResult object at 0x7f7a58dce9e8> was successful\n"
     ]
    }
   ],
   "source": [
    "# start a pool\n",
    "with mp.Pool(processes=4) as pool:\n",
    "\n",
    "    # submit a job that will run successfully\n",
    "    async0 = pool.apply_async(who, [2])\n",
    "    \n",
    "    # submit a job that will fail\n",
    "    async1 = pool.apply_async(who, ['apple'])\n",
    "    \n",
    "    # join jobs into an iterable \n",
    "    jobs = [async0, async1]\n",
    "    \n",
    "    # iterate over async results and print result of succesful jobs\n",
    "    while 1:\n",
    "        for job in jobs:\n",
    "            # skip over the job for now if not ready yet\n",
    "            if job.ready():\n",
    "                if job.successful():\n",
    "                    print('{} was successful'.format(job))\n",
    "                else:\n",
    "                    print('{} had an error'.format(job))\n",
    "                # remove job from queue\n",
    "                jobs.remove(job)\n",
    "        \n",
    "        # end loop if all jobs are finished \n",
    "        time.sleep(0.5)\n",
    "        if len(jobs) == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ipyparallel -- large-scale computing\n",
    "The libraries above have limitations for large-scale computing. Because multithreading shares the memory space among threads it is limited to the number of physically connected cores on a CPU. Which is to say that it cannot split work up across multiple CPUs or computers. Similarly, multiprocessing can split work up among CPUs on a single computer, but cannot connect multiple computers or nodes of a cluster together. To do this we need a more complex protocol. This is where `ipyparallel` comes in. \n",
    "\n",
    "This library has a powerful program called `ipcluster` for starting a cluster and connecting CPUs from across one of more computers or nodes, and then it has a `multiprocessing`-like Python API for distributing jobs across those cores. Using this tool you can connect to hundreds or thousands of cores and distribute work to them while working in an interactive jupyter notebook. \n",
    "\n",
    "Before running the code below you will need to start an ipcluster instance running in a separate terminal with the command below. If you're working on a cluster then use the terminal tab in the jupyter dashboard to open a terminal and make sure you source the proper conda environment (Py2 or Py3) that is the same as the code you are running here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipcluster start --n=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries on remote engines using px magic\n",
    "%px import time, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to a running ipcluster instance \n",
    "ipyclient = ipp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7488, 7491, 7484, 7485, 7488, 7484, 7491, 7485, 7488, 7484]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose method for distributing jobs on client\n",
    "lb = ipyclient.load_balanced_view()\n",
    "\n",
    "# send jobs\n",
    "jobs = [lb.apply(who, 1) for q in queries]\n",
    "\n",
    "# collect results\n",
    "res = [i.get() for i in jobs]\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
